{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "302ddd4c-7100-47b5-bbb7-aa62bbf04309",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Running the notebook\n",
    "To run this notebook you must install the following:  \n",
    "       -  pip install pyspark  \n",
    "       -  pip install re  \n",
    "       -  pip install os   \n",
    "       -  pip install pyspellchecker  \n",
    "       -  pip install nltk  \n",
    "       \n",
    "       \n",
    "    \n",
    "\n",
    "### Section 1: Process and Format New Tweet Data\n",
    "\n",
    "The goal of this section is to process the data retrieved by scraping Twitter using their developer API, and match the formatting of the tweet dataset used in the paper.\n",
    "\n",
    "\n",
    "#### 1.1: Formatting New DataFrames\n",
    "There are 3 columns considered to be relevant for this analysis: airline_sentiment, airline and text. The raw csv files will be formatted to match these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb0ace6e-5fea-4df1-bf4f-f9eae2f0f04e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf,lower, regexp_replace, split, concat, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .appName('my-cool-app') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "95b3b814-d2eb-4455-b456-9755857cb7ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[raw_text: string, airline_sentiment: string, airline: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##import packages\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "\n",
    "\n",
    "##Load Data and Create Dataframes\n",
    "def load_csv(airline):\n",
    "  #load csv\n",
    "    cwd = os.getcwd()\n",
    "    file_location = cwd+\"/\"+airline+\"_labelled.csv\"  \n",
    "    df = spark.read.format(\"csv\").option(\"header\", False).option(\"multiLine\", True).option(\"escape\", \"\\\"\").load(file_location)\n",
    "\n",
    "    #basic formatting of new pyspark dataframes\n",
    "    df = df.drop('_c0')\n",
    "    df = df.withColumnRenamed('_c1', 'raw_text')\n",
    "    df = df.withColumnRenamed('_c2', 'airline_sentiment')\n",
    "    return df\n",
    "\n",
    "#create dataframes, adding new column with airline name\n",
    "df_virgin_raw = load_csv(\"virgin\").withColumn(\"airline\",lit(\"Virgin America\"))\n",
    "df_delta_raw = load_csv(\"delta\").withColumn(\"airline\",lit(\"Delta\"))\n",
    "df_american_raw = load_csv(\"american_airline\").withColumn(\"airline\",lit(\"American\"))\n",
    "df_southwest_raw = load_csv(\"southwest\").withColumn(\"airline\",lit(\"Southwest\"))\n",
    "df_united_raw = load_csv(\"unitedairlines\").withColumn(\"airline\",lit(\"United\"))\n",
    "df_us_airways_raw = load_csv(\"us_airways\").withColumn(\"airline\",lit(\"US Airways\"))\n",
    "\n",
    "display(df_virgin_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06aa9ef7-e289-4a78-8994-ff9776574277",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2: Removing UTF-encoding and html links from raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9997cbb-4ab9-4342-a087-18e689e695a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#use regex to clean up raw text\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "\n",
    "##create a regex-based UDF\n",
    "def regex_cleanup(text):\n",
    "    #remove UTF-8 codes from text\n",
    "    text = re.sub(r'\\\\x[0-9][0-9]',' ',text) \n",
    "    text = re.sub(r'\\\\x[a-z][a-z]',' ',text)\n",
    "    text = re.sub(r'\\\\x[a-z][0-9]',' ',text)\n",
    "    text = re.sub(r'\\\\x[0-9][a-z]',' ',text)\n",
    "\n",
    "    text = re.sub(r\"b'RT\",' ',text) ##remove retweet(RT) characters\n",
    "    text = re.sub(r'b\"RT',' ',text) ##remove retweet(RT) characters\n",
    "    text = re.sub(r\"b'\",' ',text) ##remove \"b\" bytestring characters\n",
    "    text = re.sub(r'b\"',' ',text) ##remove \"b\" bytestring characters\n",
    "    text = re.sub(r\"\\\\n+\",' ',text) ##remove new line character\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #remove twitter https links\n",
    "    return text\n",
    "\n",
    "##convert this function to pyspark UDF\n",
    "regex_cleanup_UDF = udf(lambda x: regex_cleanup(x))\n",
    "\n",
    "##apply regex_cleanup UDF to dataframe\n",
    "def cleanup_text(df):\n",
    "    df = df.withColumn(\"text\",regex_cleanup_UDF(\"raw_text\"))\n",
    "    df = df.drop(\"raw_text\")\n",
    "    return df.dropDuplicates((['text'])) #delete duplicate rows that may occur due to retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cacf91b5-879a-4b74-9d33-c78c6097b7cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_sentiment: string, airline: string, text: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Call cleanup function on all dataframes\n",
    "df_virgin = cleanup_text(df_virgin_raw)\n",
    "df_delta = cleanup_text(df_delta_raw)\n",
    "df_american = cleanup_text(df_american_raw)\n",
    "df_southwest = cleanup_text(df_southwest_raw)\n",
    "df_united = cleanup_text(df_united_raw)\n",
    "df_us_airways = cleanup_text(df_us_airways_raw)\n",
    "\n",
    "##combine all airline dataframes into one\n",
    "temp = df_virgin.union(df_delta)\n",
    "temp = temp.union(df_american)\n",
    "temp = temp.union(df_southwest)\n",
    "temp = temp.union(df_united)\n",
    "new_data_df = temp.union(df_us_airways) #final df\n",
    "\n",
    "display(new_data_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|airline_sentiment|count|\n",
      "+-----------------+-----+\n",
      "|         positive|  127|\n",
      "|          neutral|  245|\n",
      "|         negative|  225|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_space(text):\n",
    "    text = re.sub(r'\\t','',text) \n",
    "    text = re.sub(r'\\s+','',text) \n",
    "    return text\n",
    "\n",
    "##convert this function to pyspark UDF\n",
    "remove_space_UDF = udf(lambda x: remove_space(x))\n",
    "\n",
    "##apply regex_cleanup UDF to dataframe\n",
    "def removing_spaces(df):\n",
    "    df = df.withColumn(\"airline_sentiment\",remove_space_UDF(\"airline_sentiment\"))\n",
    "    return df\n",
    "\n",
    "new_data_df = removing_spaces(new_data_df)\n",
    "new_data_df.groupBy(\"airline_sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a1107ae-476d-418c-b61d-b94e8ce7d545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_sentiment: string, airline: string, text: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_sentiment: string, airline: string, text: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Load tweet data and drop irrelevant columns\n",
    "def load_tweets(file_location):\n",
    "  #load csv\n",
    "    df = spark.read.format(\"csv\").option(\"header\", True).option(\"multiLine\", True).option(\"escape\", \"\\\"\").load(file_location)\n",
    "\n",
    "    #basic formatting of new pyspark dataframes\n",
    "    drop_list = ['tweet_id','airline_sentiment_confidence', 'negativereason', 'negativereason_confidence','airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'tweet_coord','tweet_created', 'tweet_location', 'user_timezone' ]\n",
    "    df = df.drop(*drop_list)\n",
    "\n",
    "    return df\n",
    "\n",
    "##load tweet dataset\n",
    "df_tweets = load_tweets( \"Tweets.csv\")\n",
    "display(df_tweets)\n",
    "\n",
    "##Combine dataframe for new data with dataframe for old data to create 1 dataframe\n",
    "df_final = df_tweets.union(new_data_df)\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+--------------------+\n",
      "|airline_sentiment|       airline|                text|\n",
      "+-----------------+--------------+--------------------+\n",
      "|          neutral|Virgin America|@VirginAmerica Wh...|\n",
      "|         positive|Virgin America|@VirginAmerica pl...|\n",
      "|          neutral|Virgin America|@VirginAmerica I ...|\n",
      "|         negative|Virgin America|@VirginAmerica it...|\n",
      "|         negative|Virgin America|@VirginAmerica an...|\n",
      "|         negative|Virgin America|@VirginAmerica se...|\n",
      "|         positive|Virgin America|@VirginAmerica ye...|\n",
      "|          neutral|Virgin America|@VirginAmerica Re...|\n",
      "|         positive|Virgin America|@virginamerica We...|\n",
      "|         positive|Virgin America|@VirginAmerica it...|\n",
      "+-----------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04c8c825-75d7-44ea-a65d-86749a0ffac4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3: Briefly Investigate our Data\n",
    "\n",
    "In this section we will briefly explore our dataset. We want to investigate:\n",
    "- How any original datapoints did we have?\n",
    "- How many new datapoints did we add?\n",
    "- How many total datapoints do we have now?\n",
    "- How many of each sentiment do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5cc856ec-2e86-457d-be7a-816f78fd60b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sentimentCounter(df):\n",
    "    negative = 0\n",
    "    positive = 0\n",
    "    neutral = 0\n",
    "\n",
    "    for row in df.rdd.collect():\n",
    "        if \"negative\" in row['airline_sentiment']: \n",
    "            negative = negative + 1\n",
    "        elif \"neutral\" in row['airline_sentiment']: \n",
    "            neutral = neutral + 1\n",
    "        elif \"positive\" in row['airline_sentiment']: \n",
    "            positive = positive + 1\n",
    "  \n",
    "    return negative, neutral, positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c0d272b7-df1c-46ea-a53a-a742af30dc79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new data points:  597\n",
      "Number of original data points:  14640\n",
      "Total number of data points:  15237\n",
      "\n",
      "\n",
      "Number of positives:  2490 (16.34 %)\n",
      "Number of negatives:  9403 (61.71 %)\n",
      "Number of neutrals:  3344 (21.95 %)\n"
     ]
    }
   ],
   "source": [
    "#Check: How many data points did we have, how many did we add?\n",
    "print(\"Number of new data points: \",new_data_df.count())\n",
    "print(\"Number of original data points: \",df_tweets.count())\n",
    "print(\"Total number of data points: \",df_final.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "#Check: Count and percentage of each sentiment\n",
    "negative, neutral, positive = sentimentCounter(df_final)\n",
    "total = negative + positive + neutral\n",
    "print(\"Number of positives: \",positive,\"(%.2f\"%(positive/total*100),\"%)\")\n",
    "print(\"Number of negatives: \",negative,\"(%.2f\"%(negative/total*100),\"%)\")\n",
    "print(\"Number of neutrals: \",neutral,\"(%.2f\"%(neutral/total*100),\"%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "171dcae2-cfae-4abd-bf9b-711223dc1d96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Section 2: Preprocessing Data\n",
    "The goal of this section is to clean the text by:\n",
    "  1. Converting text to lower-case\n",
    "  2. Removing any noise\n",
    "  3. Tokenizing words\n",
    "  4. Spell check words\n",
    "  5. Removing stop words\n",
    "  6. Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "375c3939-a105-4f14-9685-61f9d3940684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34b516d2-c3f2-4ce8-a1b2-b6bd3cd59c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.functions import udf, lower, regexp_replace, col\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import pyspark.sql.functions as f\n",
    "from spellchecker import SpellChecker\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b604a23-1d74-4814-8d20-394011328cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, col\n",
    "\n",
    "def clean_col(col): \n",
    "  #remove noise, any @s\n",
    "  col = re.sub(r\"@[A-z]*\", \"\",col) \n",
    "  #remove noise, more than one space\n",
    "  col= re.sub(r\" {2,}\", \" \",col) \n",
    "  #remove noise, any numbers\n",
    "  col = re.sub(r\"\\d\", \"\",col)\n",
    "  #remove punctuation\n",
    "  col = re.sub(r\"[!'#$%&'()*+,\\-./:;<=>?@\\[/\\]^_{|}~\\\"]\", \"\",col)\n",
    "  #convert tweet to lower-case\n",
    "  col = col.lower()\n",
    "  \n",
    "  return col\n",
    "\n",
    "clean_col_UDF = udf(lambda x: clean_col(x))\n",
    "df_original_regex = df_tweets.withColumn('preText1', clean_col_UDF(col('text')))\n",
    "df_new_regex = new_data_df.withColumn('preText1', clean_col_UDF(col('text')))\n",
    "df_final_regex = df_final.withColumn('preText1', clean_col_UDF(col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd436c4a-8559-4686-bf5d-898a8fe7afb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_sentiment: string, airline: string, text: string, preText1: string, words: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def token_df(df):\n",
    "  token_udf = udf(lambda words: word_tokenize(words), ArrayType(StringType()))\n",
    "  df = df.withColumn(\"words\", token_udf(col(\"preText1\")))\n",
    "  return df\n",
    "\n",
    "df_token = token_df(df_final_regex)\n",
    "df_token_original = token_df(df_original_regex)\n",
    "df_token_new = token_df(df_new_regex)\n",
    "display(df_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f41de76b-9a8e-47d2-b4fa-e5b7645ce9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[airline_sentiment: string, airline: string, text: string, preText1: string, words: array<string>, no_stop: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "def stop_words_remove_df(df):\n",
    "    remover = StopWordsRemover()\n",
    "    remover.setInputCol(\"words\")\n",
    "    remover.setOutputCol(\"no_stop\")\n",
    "    df = remover.transform(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_stop = stop_words_remove_df(df_token)\n",
    "df_new_stop = stop_words_remove_df(df_token_new)\n",
    "df_original_stop = stop_words_remove_df(df_token_original)\n",
    "\n",
    "display(df_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cee6eab-94d3-489c-8de2-6de42d845a1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def stem_words(sentence):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    reArr = []\n",
    "    for word in sentence:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        if len(stem_word) > 2:\n",
    "            reArr.append(stem_word)\n",
    "\n",
    "    return reArr\n",
    "\n",
    "def stem_col(df):\n",
    "    stemmer_udf = udf(lambda words: stem_words(words), ArrayType(StringType()))\n",
    "    col_stem = stemmer_udf(col(\"no_stop\"))\n",
    "    df = df.withColumn(\"text_final\", col_stem.cast(ArrayType(StringType())))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_final_pre = stem_col(df_stop)\n",
    "df_new_pre = stem_col(df_new_stop)\n",
    "df_original_pre = stem_col(df_original_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34abd81b-ed71-4284-956b-5b5add838044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_pre = df_final_pre.select(\"airline_sentiment\", \"airline\", \"text_final\")\n",
    "df_new_pre = df_new_pre.select(\"airline_sentiment\", \"airline\", \"text_final\")\n",
    "df_original_pre = df_original_pre.select(\"airline_sentiment\", \"airline\", \"text_final\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "114b7c83-cabc-4a56-9ab8-06db4a3793cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+--------------------+\n",
      "|airline_sentiment|       airline|          text_final|\n",
      "+-----------------+--------------+--------------------+\n",
      "|          neutral|Virgin America|              [said]|\n",
      "|         positive|Virgin America|[plus, youv, comm...|\n",
      "|          neutral|Virgin America|[didnt, today, mu...|\n",
      "|         negative|Virgin America|[realli, aggress,...|\n",
      "|         negative|Virgin America|[realli, big, bad...|\n",
      "|         negative|Virgin America|[serious, pay, fl...|\n",
      "|         positive|Virgin America|[yes, near, everi...|\n",
      "|          neutral|Virgin America|[realli, miss, pr...|\n",
      "|         positive|Virgin America|   [well, didnt…but]|\n",
      "|         positive|Virgin America|[amaz, arriv, hou...|\n",
      "+-----------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final_pre.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60dd2052-ce34-453a-8718-94649242d115",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 3\n",
    "The goal of this section is to apply machine learning to get the tweet sentiment by:  \n",
    "1. Using HashingTF to create the features  \n",
    "2. Applyiing idf to the features\n",
    "3. Creating a pipeline  \n",
    "4. Creating a parameter grid to tune the hyperparameters\n",
    "5. Training the data\n",
    "6. Applying Logistic regression and Random Forest\n",
    "7. Comparing our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using HashingTF to create features\n",
    "#apply IDF to features\n",
    "#pipeline used to execute all these steps at once\n",
    "def create_IDF_vector(df):\n",
    "    hashtf = HashingTF(numFeatures=1000, inputCol=\"text_final\", outputCol='tf')\n",
    "    idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) \n",
    "    label_stringIdx = StringIndexer(inputCol = \"airline_sentiment\", outputCol = \"label\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=[hashtf, idf, label_stringIdx])\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    pipeline_df = pipeline_model.transform(df)\n",
    "    \n",
    "    return pipeline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model and return predictions\n",
    "def train_model(df,model):\n",
    "    (train_set,test_set) = df.randomSplit([0.80, 0.20],seed = 2000)\n",
    "    trained_model = model.fit(train_set)\n",
    "    predictions = trained_model.transform(test_set)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instances of ML models\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "#create vectors\n",
    "vector_df = create_IDF_vector(df_final_pre)\n",
    "vector_df_new = create_IDF_vector(df_new_pre)\n",
    "vector_df_original = create_IDF_vector(df_original_pre)\n",
    "\n",
    "#get predictions\n",
    "rf_predictions = train_model(vector_df,rf)\n",
    "lr_predictions = train_model(vector_df,lr)\n",
    "rf_predictions_new = train_model(vector_df_new, rf)\n",
    "lr_predictions_new = train_model(vector_df_new, lr)\n",
    "rf_predictions_original = train_model(vector_df_original, rf)\n",
    "lr_predictions_original = train_model(vector_df_original, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_metrics(predictions):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1_score = evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(\"f1 score: \",f1_score)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    precision = evaluator.evaluate(predictions)\n",
    "    print(\"Precision: \",precision)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    recall = evaluator.evaluate(predictions)\n",
    "    print(\"Recall: \",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Logistic Regression Metrics For All Data*****\n",
      "f1 score:  0.7210887831987325\n",
      "Precision:  0.7178907590834822\n",
      "Recall:  0.7256874580818243\n",
      "\n",
      "\n",
      "\n",
      "***** Random Forest Metrics For all Data*****\n",
      "f1 score:  0.47795888785908264\n",
      "Precision:  0.38780064693999006\n",
      "Recall:  0.6227364185110664\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Logistic Regression Metrics For All Data*****\")\n",
    "get_classification_metrics(lr_predictions)\n",
    "print(\"\\n\\n\")\n",
    "print(\"***** Random Forest Metrics For all Data*****\")\n",
    "get_classification_metrics(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Logistic Regression Metrics For New Data*****\n",
      "f1 score:  0.49374416433239965\n",
      "Precision:  0.5138726033048878\n",
      "Recall:  0.49206349206349204\n",
      "\n",
      "\n",
      "\n",
      "***** Random Forest Metrics For New Data*****\n",
      "f1 score:  0.43885390843231786\n",
      "Precision:  0.626732174351222\n",
      "Recall:  0.4682539682539682\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Logistic Regression Metrics For New Data*****\")\n",
    "get_classification_metrics(lr_predictions_new)\n",
    "print(\"\\n\\n\")\n",
    "print(\"***** Random Forest Metrics For New Data*****\")\n",
    "get_classification_metrics(rf_predictions_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Logistic Regression Metrics For Original Data*****\n",
      "f1 score:  0.7230469792435031\n",
      "Precision:  0.7196853866055846\n",
      "Recall:  0.7279411764705882\n",
      "\n",
      "\n",
      "\n",
      "***** Random Forest Metrics For Original Data*****\n",
      "f1 score:  0.4881821341675356\n",
      "Precision:  0.3981009070294784\n",
      "Recall:  0.6309523809523809\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Logistic Regression Metrics For Original Data*****\")\n",
    "get_classification_metrics(lr_predictions_original)\n",
    "print(\"\\n\\n\")\n",
    "print(\"***** Random Forest Metrics For Original Data*****\")\n",
    "get_classification_metrics(rf_predictions_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set,test_set) = vector_df.randomSplit([0.80, 0.20],seed = 2000)\n",
    "(train_set_new,test_set_new) = vector_df_new.randomSplit([0.80, 0.20],seed = 2000)\n",
    "(train_set_original,test_set_original) = vector_df_original.randomSplit([0.80, 0.20],seed = 2000)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do grid search for LogisticRegression\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01,0.1,1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3) \n",
    "\n",
    "cvModel_lr = crossval.fit(train_set)\n",
    "cvModel_lr_new = crossval.fit(train_set_new)\n",
    "cvModel_lr_original = crossval.fit(train_set_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do grid search for RandomForestClassifier\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [1,3,5,10])\n",
    "             .addGrid(rf.maxDepth, [3,5,7,10])\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3) \n",
    "\n",
    "cvModel_rf = crossval.fit(train_set)\n",
    "cvModel_rf_new = crossval.fit(train_set_new)\n",
    "cvModel_rf_original = crossval.fit(train_set_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Best parameters for Logistic Regression for all data **********\n",
      "Best regParam:  0.01\n",
      "Best maxIter:  10\n",
      "\n",
      "\n",
      "********** Best parameters for Random Forest for all data **********\n",
      "Best number of trees:  RandomForestClassificationModel: uid=RandomForestClassifier_315be220be4b, numTrees=1, numClasses=3, numFeatures=1000\n",
      "Best max depth:  10\n"
     ]
    }
   ],
   "source": [
    "best_model_lr = cvModel_lr.bestModel\n",
    "best_model_rf = cvModel_rf.bestModel\n",
    "\n",
    "print(\"********** Best parameters for Logistic Regression for all data **********\")\n",
    "print('Best regParam: ',best_model_lr.getRegParam())\n",
    "print('Best maxIter: ',best_model_lr.getMaxIter())\n",
    "\n",
    "print(\"\\n\\n********** Best parameters for Random Forest for all data **********\")\n",
    "print('Best number of trees: ',best_model_rf)\n",
    "print('Best max depth: ',best_model_rf.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_LR_prediction = cvModel_lr.transform(test_set)\n",
    "best_RF_prediction = cvModel_rf.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Logistic Regression Metrics (Best Model) for all data *****\n",
      "f1 score:  0.7173895174221085\n",
      "Precision:  0.713303102710151\n",
      "Recall:  0.7256874580818242\n",
      "\n",
      "\n",
      "***** Random Forest Metrics (Best Model) for all data *****\n",
      "f1 score:  0.5994190525967966\n",
      "Precision:  0.6334523089472892\n",
      "Recall:  0.6720321931589538\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Logistic Regression Metrics (Best Model) for all data *****\")\n",
    "get_classification_metrics(best_LR_prediction)\n",
    "print(\"\\n\")\n",
    "print(\"***** Random Forest Metrics (Best Model) for all data *****\")\n",
    "get_classification_metrics(best_RF_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "def create_confusion_matrix(df):\n",
    "    label_pred_new = df.select(['label', 'prediction'])\n",
    "    metrics = MulticlassMetrics(label_pred_new.rdd.map(tuple))\n",
    "    array = metrics.confusionMatrix().toArray()\n",
    "    print(metrics.confusionMatrix().toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26. 12.  5.]\n",
      " [20. 22.  9.]\n",
      " [13.  5. 14.]]\n"
     ]
    }
   ],
   "source": [
    "#New data confusion matrix\n",
    "create_confusion_matrix(lr_predictions_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1536.  237.  123.]\n",
      " [ 190.  252.   72.]\n",
      " [  76.   79.  291.]]\n"
     ]
    }
   ],
   "source": [
    "#Original data confusion matrix\n",
    "create_confusion_matrix(lr_predictions_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1579.  240.  132.]\n",
      " [ 193.  285.   80.]\n",
      " [  85.   88.  300.]]\n"
     ]
    }
   ],
   "source": [
    "#All data confusion matrix\n",
    "create_confusion_matrix(lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1607.  269.  143.]\n",
      " [ 172.  265.   77.]\n",
      " [  78.   79.  292.]]\n"
     ]
    }
   ],
   "source": [
    "#All data after tuning of hyperparameters\n",
    "create_confusion_matrix(best_LR_prediction)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "612 Project",
   "notebookOrigID": 1453616040331399,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
